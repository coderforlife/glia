#!/usr/bin/env python
"""
GLIA Tasks Wrapper.
"""

from __future__ import division
from __future__ import unicode_literals
from __future__ import absolute_import
from __future__ import print_function

def check_image_stack(tasks, parser, args, name, shapes=None, conv=('--convert', 'dt=F32')):
    """
    Checks an input image stack and also creats a task for that stack to be converted to
    MHA files.
    
    Parameters:
      tasks     the Tasks object to add the conversion task to
      parser    the ArgumentParser object to send errors to
      args      the arguments for the program generated by parser
      name      the name of the image stack argument, also the output folder will be in
                args.temp/name[:-1]
      shapes    if provided will make sure that the shape of this image stack matches the
                given shapes
      conv      the conversion commands to send to imstack, defaults to converting to
                32-bit floating-point images
    Returns: the shapes
    """
    from os import getcwd
    from os.path import join, relpath
    from itertools import izip
    from pysegtools.tasks import MB

    # Get information from the image stack
    cmd,ims = getattr(args, name)
    files  = [relpath(f, args.temp) for f in ims.filenames] # TODO: use os.path.commonprefix to improve results
    if shapes is None:
        shapes = [im.shape for im in ims]
    elif len(ims) != len(shapes) or any(im.shape != sh for im,sh in izip(ims, shapes)):
        parser.error("all input image stacks must be the same shape (ims and %s not equal)"%name)
    if args.threeD and not ims.is_homogeneous:
        parser.error("when operating in 3D the input image stacks must be homogeneous")
    npxls  = max(w*h for w,h in shapes)
    nbytes = max(im.dtype.itemsize*im.shape[0]*im.shape[1] for im in ims)
    ims.close()
   
    # Add the conversion task
    folder = name[:-1]
    save,out = (folder+'.mha', join(folder+'.mha')) if args.threeD else \
               (join(folder, '####.mha'), [join(folder, '%04d.mha'%i) for i in xrange(len(files))])
    tasks.add(('imstack', '-L', cmd, conv, '-S', join(args.temp, save), 'CompressedData=true'),
              inputs=files, outputs=out, wd=getcwd(), mem=20*MB+nbytes+4*npxls)
    
    return shapes

def check_image_stacks(tasks, parser, args):
    """
    Checks all of the image stack arguments and also adds conversion to MHA tasks for them.
    Returns the number of images in the stacks.
    """
    from os import getcwd
    from os.path import join
    shapes = check_image_stack(tasks, parser, args, 'ims') # raw images
    check_image_stack(tasks, parser, args, 'pbs', shapes) # probability maps
    if args.msks is not None:
        check_image_stack(tasks, parser, args, 'msks', shapes, ('-t', '1')) # masks
    if args.training:
        check_image_stack(tasks, parser, args, 'lbls', shapes, ('--renumber', 'per-slice=false')) # labels
    if args.outs is not None:
        load,ins = ('out.mha', 'out.mha') if args.threeD else \
                   (join('out', '####.mha'), [join('out', '%04d.mha'%i) for i in xrange(len(shapes))])
        tasks.add(('imstack', '-L', join(args.temp, load), '--shrink-int', 'per-slice=False', '-S') + tuple(args.outs),
                  inputs=ins, wd=getcwd())
        # TODO: mem=?
        # TODO: outputs=relpath(args.outs, temp) # don't have the output path(s)
    elif not args.training: parser.error("Must provide outs image stack if not training")
    return len(shapes)

def get_cmds():
    from ._utils import int_check, float_check
    from ._commands import SingleCmd, RepeatedCmd
    from ._commands import InputSet, InputImageStack, OutputImageStack, InputSeries, OutputSeries, Model, Condition
    from ._commands import Parameter, ListParameter, EnumParameter, BoolParameter, Masks
    
    # TODO: memory values for programs
    cmds = []

    no_blur = lambda s,a:(s.get('sigma',1) == 0)
    
    # Prep: Image Blurring
    cmds.append(SingleCmd('imstack',
        '-L', InputImageStack('pb', 'mha'),
        '--gaussian-blur', Parameter('&sigma', 1.0, float_check(0.0), 'the amount of Gaussian blur to use during some steps, default is 1.0 while 0.0 turns off blurring'),
        '-S', OutputImageStack('pbb', 'mha'), 'CompressedData=true',
        skip=no_blur)) # TODO: mem=20*MB+6*pxls

    # TODO: how to do this in the new system?
    # 0: Training texures
    #ts.add(('imstack', '-L', ..., '--gaussian-blur', str(sigma), '--convert', 'dt=F32', '-S', t_im_blur_gen), inputs=t_im, outputs=t_im_blur, settings='sigma', mem=20*MB+bytes_t+4*pxls_t)
    #ts.add(('hnsGenTextonDict', [('-v', v, '-l', l) for v, l in izip(t_im_blur, t_s_clr_mha)], 'textondict.ssv'), inputs=t_im_blur+t_s_clr_mha, outputs='textondict.ssv', cpu=0.7)

    # 1: Initial segmentation (watershed)
    #if wl is None:
    #    # TODO: create calculate_watershed_water_level and update watershed to accept waterlevel as a file (repeated below)
    #    ts.add(('calculate_watershed_water_level', 0, n, 't_pbb/####.mha', 't_lbl/####.mha', areaThresholds, probThreshold, wl_file),
    #           inputs=..., outputs=..., settings=('water-lvl', 'pm-area-thresholds', 'pm-prob-threshold'), cpu=4)
    cmds.append(RepeatedCmd('watershed',
        '-i', Condition(no_blur, InputSeries('pb', 'mha'), InputSeries('pbb', 'mha')),
        # TODO: dynamic water-lvl: Parameter('&water-lvl', None, ..., '...if not provided will calculate an acceptable value')
        '-l', Parameter('&water-lvl', 0.01, float_check(0.0, 1.0), 'the watershed water level parameter, likely <=0.02, default is 0.01'),
        '-o', OutputSeries('is1', 'mha'), '-z', 'true'))

    # 2: Pre-merging
    cmds.append(RepeatedCmd('pre_merge',
        '-s', InputSeries('is1', 'mha'), '-p', InputSeries('pb', 'mha'), Masks(),
        # TODO: default [50,100]? or just [50]?
        '-t', ListParameter('pm-area-thresh', [50,200], int_check(1), (1,2), 'pre-merge area thresholds as 1 or 2 integers, default is 50 200'),
        '-b', Parameter('pm-prob-thresh', 0.5, float_check(0.0, 1.0), 'pre-merge average probability thresh, default is 0.5'),
        '-o', OutputSeries('is2', 'mha'), '-z', 'true'))

    # 3: Merge order and saliencies
    cmds.append(RepeatedCmd('merge_order_pb',
        '-s', InputSeries('is2', 'mha'), '-p', InputSeries('pb', 'mha'), Masks(),
        '-t', EnumParameter('mop-boundary-intensity-stats', 'median', ('median','mean'), 'merge order boundary intensity stats type, default is median (other option is mean)'),
        '-o', OutputSeries('ord', 'ssv'), '-y', OutputSeries('sal', 'ssv')))

    # 4: Boundary condition features
    cmds.append(RepeatedCmd('bc_feat',
        '-s', InputSeries('is2', 'mha'), '--pb', InputSeries('pb', 'mha'), Masks(),
        '-o', InputSeries('ord', 'ssv'), '-y', InputSeries('sal', 'ssv'),
        '--rbi', InputSeries('im', 'mha'), '--rbb', '16', '--rbl', '0.0', '--rbu', '1.0',
        '--rbi', InputSeries('pb', 'mha'), '--rbb', '16', '--rbl', '0.0', '--rbu', '1.0',
        # TODO: texton: '--rli', InputSeries('tx', 'mha), '--rlb', '100', '--rll', '0', '--rlu', '99',
        '--s0', Parameter('bcf-init-sal', 1.0, float, 'boundary classifier initial saliency, default is 1.0'),
        '--sb', Parameter('bcf-sal-bias', 1.0, float, 'boundary classifier saliency bias, default is 1.0'),
        '--bt', ListParameter('bcf-shape-thresholds', [0.2,0.5,0.8], float, '+', 'boundary classifier thresholds for shape features, default is 0.2 0.5 0.8'),
        '-n', BoolParameter('bcf-norm-shape', 'boundary classifier features normalize size and length'),
        '-l', BoolParameter('bcf-shape-log', 'boundary classifier uses logs of shape as features'),
        '--simpf', BoolParameter('bcf-simp-feats', 'boundary classifier only use simplified features'),
        # TODO: more parameters like histogram settings
        # TODO: '-r', OutputSeries('rcf', 'ssv'), -- region features file
        '-b', OutputSeries('bcf', 'ssv'))) # TODO: run_on_cluster=True

    # 5: Boundary classifier label generation
    #   TODO:
    #   --f1 false	use traditional Rand index instead of pair F1
    #   -g int		global optimal assignment type (0: none [default], 1: RCC by merge, 2: RCC by split)
    #   -p true		determine based on optimal splits
    #   -w true		tweak conditions for thick boundaries
    #   -d float	maximum precision drop allowed for merge
    cmds.append(RepeatedCmd('bc_label_ri',
        '-s', InputSeries('is2', 'mha'), '-o', InputSeries('ord', 'ssv'), '-t', InputSeries('lbl', 'mha'), Masks(),
        '-p', 'false', '-w', 'true',
        '-l', OutputSeries('bcl', 'ssv'),
        skip=lambda s,a:not a.training))

    # 6: Training Data Generation
    #   TODO:
    #	-b		    balance positive and negative sample counts
    #   -N
    #   -M
    #   -C
    #   -K
    #   -B
    #   -n
    #   -R
    cmds.append(SingleCmd('python', '-m', 'glia.train_ldnn',
        Model(False), '-f', InputSet('bcf', 'ssv'), '-l', InputSet('bcl', 'ssv'), '--append',
        skip=lambda s,a:not a.training))

    # 7: Generate Predictions
    cmds.append(RepeatedCmd('python', '-m', 'glia.test_ldnn',
        Model(True), InputSeries('bcf', 'ssv'), OutputSeries('bcp', 'ssv'), '--flip-preds',
        skip=lambda s,a:(a.outs is None)))

    # 8: Segmentation
    #   TODO:
    #	-n file		region probability file
    #	-i false	do not ignore missing regions by labeling them BG_VAL
    #	-r true		relabel output image to consecutive labels
    #	-b file		boundary confidence image output
    cmds.append(RepeatedCmd('segment_greedy',
        '-s', InputSeries('is2', 'mha'), '-o', InputSeries('ord', 'ssv'), '-p', InputSeries('bcp', 'ssv'), Masks(),
        '-r', 'true', '-z', 'true',
        '-f', OutputSeries('out', 'mha'),
        skip=lambda s,a:(a.outs is None)))

    return cmds

from argparse import HelpFormatter

class MyHelpFormatter(HelpFormatter):
    def _format_usage(self, usage, actions, groups, prefix):
        import re
        from argparse import Action
        if prefix is None: prefix = 'usage: '

        # if usage is specified, use that
        if usage is not None: return '%s%s\n\n' % (prefix, usage % {'prog':self._prog})

        # if no optionals or positionals are available, usage is just prog
        if not actions: return '%s%s\n\n' % (prefix, '%(prog)s' % {'prog':self._prog})

        # if optionals and positionals are available, calculate usage
        prog = '%(prog)s' % {'prog':self._prog}

        # Condense the really long parameters into a single argument
        has_params = False
        new_actions = []
        for action in actions:
            show = (not action.option_strings or action.required or
                    any(len(s) <= 2 or s[:2] != '--' for s in action.option_strings))
            if show: new_actions.append(action)
            elif not has_params:
                has_params = True
                new_actions.append(Action(['params ...'], '', required=False))
        actions = new_actions
        
        # build full usage string
        action_usage = self._format_actions_usage(actions, groups)
        usage = (prog + ' ' + action_usage).strip()

        # wrap the usage parts if it's too long
        text_width = self._width - self._current_indent
        if len(prefix) + len(usage) > text_width:

            # break usage into wrappable parts
            parts = re.findall(r'\(.*?\)+|\[.*?\]+|\S+', action_usage)
            assert ' '.join(parts) == action_usage

            # helper for wrapping lines
            def get_lines(parts, indent, prefix=None):
                lines,line = [],[]
                line_len = len(indent if prefix is None else prefix) - 1
                for part in parts:
                    if line_len + 1 + len(part) > text_width:
                        lines.append(indent + ' '.join(line))
                        line = []
                        line_len = len(indent) - 1
                    line.append(part)
                    line_len += len(part) + 1
                if line: lines.append(indent + ' '.join(line))
                if prefix is not None:
                    lines[0] = lines[0][len(indent):]
                return lines

            # if prog is short, follow it with optionals or positionals
            if len(prefix) + len(prog) <= 0.75 * text_width:
                lines = get_lines([prog] + parts, ' ' * (len(prefix) + len(prog) + 1), prefix) if parts else [prog]

            # if prog is long, put it on its own line
            else:
                lines = [prog] + get_lines(parts, ' ' * len(prefix))

            # join lines into usage
            usage = '\n'.join(lines)

        # prefix with 'usage:'
        return '%s%s\n\n' % (prefix, usage)

def __add_image_stacks_args(parser, training):
    from ._utils import open_image_stack, open_image_stack_lbl, create_image_stack
    group = parser.add_argument_group('image stacks',
        description="accepts any value accepted by `imstack -L` except outs which takes any value accepted by `imstack -S`, if the image stack value requires spaces (e.g. [ file1.png file2.png ]) then the argument needs to be in quotes")
    group.add_argument('ims', type=open_image_stack,
        help='histogram equalized grayscale raw images')
    group.add_argument('pbs', type=open_image_stack,
        help='grayscale probability maps like those generated by CHM test or -o argument of CHM train')
    group.add_argument('lbls', type=open_image_stack_lbl,
            help='RGB or grayscale images where every region is given a unique color and the unlabeled regions are 0 or black')
    group.add_argument('outs', type=create_image_stack, nargs='?' if training else None,
        help='grayscale label images where every region is given a unique value and the unlabeled regions is 0')
    group.add_argument('-m', '--mask', dest='msks', type=open_image_stack,
        help='optional B&W images to use as a mask for the ims/pbs/lbls/outs (black pixels will be dropped)')

def __add_other_options_args(parser):
    from ._utils import int_check, temp_dir, rusage_log
    from multiprocessing import cpu_count
    group = parser.add_argument_group('other options')
    group.add_argument('-h', '--help', action='help', help='show this help message and exit')
    group.add_argument('-t', '--temp', default='./temp', type=temp_dir,
        help='temporary directory, default is ./temp')
    group.add_argument('-j', '--jobs', default=cpu_count(), type=int_check(1),
        help='maximum number of local jobs to do at once, default is number of processors')
    group.add_argument('-u', '--rusage', default=None, type=rusage_log,
        help='save the resource usage information (memory and time) for each run process to a file')
    # TODO: group.add_argument('-C', '--cluster', help='use the cluster specified by the file for some operations, defaults to running everything locally', type=...)

def __create_argparser(cmds):
    from argparse import ArgumentParser
    
    # Setup the parser and the sub-parsers
    parser = ArgumentParser(description='GLIA - Graph Learning Library for Image Analysis',
                            fromfile_prefix_chars='@', formatter_class=MyHelpFormatter)
    subparsers = parser.add_subparsers(title='subcommands')
    train = subparsers.add_parser('train', description='Runs GLIA-train to create a model to segment cells in image data',
                                  add_help=False, fromfile_prefix_chars='@', formatter_class=MyHelpFormatter)
    train.set_defaults(training=True)
    test = subparsers.add_parser('test', description='Runs GLIA-test to use a model to segment cells in image data',
                                 add_help=False, fromfile_prefix_chars='@', formatter_class=MyHelpFormatter)
    test.set_defaults(training=False)
    
    # Add the model argument
    train.add_argument_group('model').add_argument('model', help='the file to write the model to')
    test.add_argument_group('model').add_argument('model', help='the file to read the model from')
    
    ##### Image Stacks #####
    __add_image_stacks_args(train, True)
    __add_image_stacks_args(test, False)

    ##### Training Parameters #####
    params = train.add_argument_group('training parameters')
    params.add_argument('-3', help='operate on a 3D volume instead of on 2D slices, the input must be a volume instead of disjoint 2D slices but the segmentation will be consistent across slices and likely greatly improved, a model trained in 3D can only be used to test volumes - NOTE: this requires that the binaries be compiled for 3D support', action='store_true', dest='threeD')
    for cmd in cmds: cmd.add_parser_arg(params)

    ##### Other Options #####
    __add_other_options_args(train)
    __add_other_options_args(test)
    
    return parser

def main():
    # TODO: waterlevel is None needs to error out for now and sigma == 0 needs to be fixed
    
    # TODO: 3D fixes needed:
    #    InputSet, InputImageStack, OutputImageStack, InputSeries, OutputSeries, Model, RepeatedCmd
    
    #fix_std_buffering()
    
    from pysegtools.tasks import Tasks
    tasks = Tasks()
    cmds = get_cmds()
    parser = __create_argparser(cmds)
    args = parser.parse_args()
        
    # Get the settings
    from pysegtools.general.json import save as save_json, load as load_json
    if args.training:
        from ._commands import unique
        settings = unique(s for cmd in cmds for s in cmd.settings)
        settings = model = {s.replace('_', '-'):getattr(args, s.replace('-', '_')) for s in settings}
        settings['threeD'] = args.threeD # 3D setting is special and needs to be copied by itself
        try:
            old = load_json(args.model)
            old.update(model)
            model = old
        except Exception: pass #pylint: disable=broad-except
        save_json(args.model, model)
    else:
        settings = load_json(args.model)
        args.threeD = settings['threeD'] # the 3D setting is special and needs to be set back into args from the settings
    
    # Check the image stacks
    n = check_image_stacks(tasks, parser, args)
    if args.threeD: n = 1

    # Apply the settings
    for cmd in cmds: cmd.apply_settings(settings, args)
    
    # Create the folders
    if not args.threeD:
        from pysegtools.general.utils import make_dir
        from os.path import join
        if any(hasattr(arg, 'folder') and not make_dir(join(args.temp, arg.folder))
               for cmd in cmds for arg in cmd.arguments):
            parser.error("error creating temporary directories")
    
    # Add tasks
    for cmd in cmds: cmd.add_tasks(tasks, n)
    
    # Run!
    tasks.run('memseg.log', args.rusage, True, settings, args.temp, args.jobs)

if __name__ == "__main__": main()

